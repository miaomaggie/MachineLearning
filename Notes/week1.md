
# 什么是机器学习
我们每天都在接触机器学习，比如我们使用百度或者谷歌搜索，就使用了学习算法；拍照时能够识别出头像，也是机器学习；邮件的垃圾分类，也是学习算法。

除了计算机领域，其他领域也会应用到机器学习。

*生物领域*收集到大量的基因数据序列、DNA序列等，使用机器算法可以更好地了解人类基因组。

*医疗领域*有公司研究出使用AI技术识别癌症。

还有我们常用的淘宝、京东，给我们推荐适合自己的物品，也得益于推荐算法，也是机器学习的一种。

人脸识别、指纹解锁也是机器学习。

可见，机器学习已经在各个领域有所应用，并且会为人类所用。

**机器学习的定义**

Arthur Samuel定义机器学习为：在进行特定编程的情况下，给予计算机学习能力的领域。当时他写了一个程序，让机器下西洋棋，机器下了上万次棋，知道什么布局赢的概率大，他的水平就超越了Samuel。

卡内基梅隆大学的Tom Mitchell提出机器学习是：一个程序被认为能从经验E 中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E后，经过P评判，程序在处理T时的性能有所提升。

对西洋棋的例子来说，我认为经验E 就是程序上万次的自我练习的经验而任务T 就是下棋。性能度量值P呢，就是它在与一些新的对手比赛时，赢得比赛的概率。

主要的算法分为了监督学习和非监督学习。监督学习是我们教机器去学习，非监督学习是让机器自己去学习。 

# 监督学习
先举几个例子。

预测房价。现有一组数据，可视化为横轴是房子面积，纵轴是房价的图。假设自己有个150平的房子，如果卖掉房子，能卖多少钱呢？

![](/images/1.jpg)

可以做一条直线，可能算出能卖150万。也可以做一条二次方程曲线，房子可能卖200万。这是两种不同的方案，哪个会更好？后面会讲到。

再举一个例子。现在有些病历，想通过病历中的肿瘤来推测乳腺癌的良性与否。

![19bb64c450e1d91850cf68ba723e5ff0.png](en-resource://database/8643:1)

纵轴表示良性与否，横轴表示肿瘤大小。1表示是恶性肿瘤，0表示良性。我们也可以不用0 1来表示，也可以用○和×来表示，为了预测肿瘤是否是良性的，也会有多个特征，除了大小之外，可能还有患者年龄，肿瘤细胞尺寸等特征。

![2ac1543bdb98c279ecf2800d4c3c130c.png](en-resource://database/8645:1)

**监督学习**的基本思想是，我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测，就像房子和肿瘤的例子中做的那样。


# 非监督学习

在监督学习中，我们已经明确知道数据的分类，已经明确知道属于什么了，良性或者恶性。

而非监督学习，则是数据中没有任何的标签，也可能都是相同的标签。那能根据这些数据了解数据集的结构吗？

无监督学习就可以判断出数据属于不同的聚集簇。这就是后面要学习的聚类算法。

![5d843f7b564d856fcf5e37dfa909fb3b.png](en-resource://database/8639:1)

谷歌新闻中应用的就是聚类算法。每天都有大量新闻产生，将新闻分组，组成有关联的新闻，形成具有同一主题的新闻。

# 代价函数

我们以线性回归的例子认识代价函数。

回到刚才预测房价的那个例子中，我们很容易就可以想到用拟合的方式对朋友的房价进行预测。我们有一组北京市房价的数据集，包括面积以及面积对应的售价，当朋友有个150平的房子需要卖时，预测能够卖出多少钱。

![6ea63febd9f3b0c9864086a3fca98556.png](en-resource://database/8649:1)

如果我们得到了一个模型，是一条直线，根据直线上的参数，我们能够预测到150平的房子卖多少钱。

整个流程可以用下图表示，我们定义$h$表示学习算法的解决方案或函数为假设，因为有多个解决方案。

![0ec7aafe13aefeb62b09bc3a9ce7c7b7.png](en-resource://database/8651:1)


一种可能的表达方式是$h_\theta(x) = \theta_0 + \theta_1x$，其中$\theta_i$表示参数。问题来了，我们该怎么选择参数呢？

![0bdb0610e7f5d900754441d1e59c5423.png](en-resource://database/8653:1)

预测值与实际值之间的差距称为**建模误差**，目标是选择出使得建模误差的平方和最小的模型参数，即使得**代价函数**$J \left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$最小，其中$m$为训练样本的数量。

![739efe43f5305424bba6c69a4bca0fd4.png](en-resource://database/8655:1)

绘制$J \left( \theta_0, \theta_1 \right)$关于参数的等高线，可以看到，总能找到使得$J$最小的点。

![5880dab93cee205255095af261c6ca91.png](en-resource://database/8657:1)

**直观理解代价函数**

![dd72e0fbe5bbe109eeb9ae2a5a1a2fca.png](en-resource://database/8659:1)

![bb14b4bdf2c2bdf920d95408df38b157.png](en-resource://database/8661:1)

![f80efa6cc8a4be7b020348aa7c035987.png](en-resource://database/8663:1)

通过上面几张图，我们可以直观理解代价函数，总能找到最小的点。当遇到的数据更复杂，维度更高时，需要找到一种方法，自动找到代价函数的最小点。

# 梯度下降

梯度下降是一个求函数最小值的算法，可以使用梯度下降法求代价函数$J \left( \theta_0, \theta_1 \right)$。

**思想**：随机选择参数组合$\left( \theta_0, \theta_1, ..., \theta_n \right)$，计算代价函数；然后寻找一个让代价函数下降最多的参数组合，直到能够达到局部最优解。下降最多的方向是负梯度方向。

当函数为凸函数时，能找到全局最优解。而一般地，只能找到局部最优解。并且，当初始值不同时，局部最优解也会不同。

![af56816d48caa9834000b7674aecc983.png](en-resource://database/8665:1)

梯度下降法算法：

![435ce85602bb9f9f405e5158e8165d95.png](en-resource://database/8667:1)

这里的$\alpha$称为学习率（learning rate），它决定了延下降程度最大方向下降的步幅有多大。

梯度下降算法公式：

$\theta_j: = \theta_j - \alpha \frac{\partial }{\partial {\theta_j}}J \left(\theta \right)$

![cb60f4e7cbc8681b8766936dea1fa4f8.png](en-resource://database/8669:1)

当$\alpha$过小时，下降速度会很慢，需要很多步才能达到最优解；而如果很大，则有可能会越过最优解的地方，导致不收敛。

以代价函数举例，说明梯度下降法的直观表示。$\alpha$保持不变。

![be6763cab45b69ee4db12be4ec447225.png](en-resource://database/8671:1)

最初是在玫红色的那个点的地方，求出导数，很陡，下降很快。更新一步之后，到了绿色的点这里，求出斜率，不那么陡了，会下降一小步。当不那么陡时，更新的幅度会小一些，越来越接近最低点。

当两者更新幅度接近零时，已经收敛到局部最优解的地方了。

这种方法也叫批量梯度下降，是因为在梯度下降时，用到了所有样本集中的数据。

**梯度下降求导**

![e7456456a59c52ba627ec981a875741a.png](en-resource://database/8783:1)

![5eef3085e15577b5a0e5426c99e2d382.png](en-resource://database/8785:1)

则算法改写成

![f9ec2314e272926d78fffc2432a5d1f0.png](en-resource://database/8787:1)

**为什么负梯度是下降最快的方向**

用泰克展开可以证明这个问题。我们设函数$f(x)$，要找到局部最小点，是通过寻找一系列$x_0, , x_1, ..., x_n$，使得$f(x_{t+1}) < f(x_t)$

![b69b0b76476fbf23331762e608bed273.png](en-resource://database/8913:1)

以一元函数为例，$f(x + \Delta x) 
\approx f(x) + \Delta x
\nabla f(x)$

如果想要$f(x+\delta x) < f(x)$，则需要$\Delta x\nabla f(x) < 0$，即令$\Delta x = -\alpha \nabla f(x)$则可更新每一步的$x$，这里的$\alpha > 0$，即学习率，也叫步长。

```math
f(x_{t+1}) < f(x_t)
```


**Notes：**

单变量线性回归假设函数是一条直线，与真实值的差值平方和误差作为代价函数。

单变量线性回归的代价函数，直观理解是关于参数的等高线图，利用梯度下降法可以求得局部最优解。

markdown公式大全可参考博文[markdown公式符号大全](
https://blog.csdn.net/konglongdanfo1/article/details/85204312)。

coursera的课程试了几种方式都看不了视频，所幸强大的bilibili有视频，[请戳](
https://www.bilibili.com/video/av63559648?p=1)。











